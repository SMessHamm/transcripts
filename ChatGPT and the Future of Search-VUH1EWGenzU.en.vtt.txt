 is chat GPT the end of search and is it  the end for Google search and what does  it all mean for seos  in this presentation I will break down  my answer for those questions and  explain why I answer the way I do so  let's get going chat GPT and the future  of search  so long term does Google care about AI  generated content the answer is no  they don't care about it and that is  because in the limit  AI generated content can be assumed to  be as good or better than what humans  can produce and when that is true  Google will not favor human content  Google will do what Google has always  done which is favor the best content  so their official stance is that they  don't care  now that is the long-term answer  the short-term answer is that AI is not  as good as humans  so when you use AI to mass produce  content for the purpose of gaming SEO  results it is considered black hat it is  considered spamming and Google frowns  upon that in the short term  so the best content therefore has what  is called a human in the loop so let's  get that out of the way  now it is important when we're talking  about these topics that we get a little  bit of context here a little bit of  extra information what is a GPT there's  three letters in there the G stands for  generative P for pre-trained and T for  Transformer and generative just means  that it generates a sequence a sequence  of tokens pre-trained means that it was  trained up to a certain moment in time  that means after another Moment In Time  the model does not know anything anymore  so it may be at 3.5 knows stuff up to  quarter four of 2021 and nothing of the  world after that so if the aliens landed  on the planet tomorrow GPT would still  write as if that had not happened yet  Transformer now this is the very  important innovation here first invented  in the paper attention is all you need  and we'll talk a little bit more about  attention in a moment but Transformers  excel at sequence to sequence problem  solving so put one sequence in and get  another sequence Out protein DNA music  can be converted in a sequence and even  text which is where a GPT is most used  for and so why is this a tension  mechanism so important it is because  attention is the foundation of  intelligence so why is attention so  powerful we all know that if we lack  attention or we have an overload of  attention or human brain shuts down and  the idea is that when you teach a  computer algorithm to have attention  it now becomes focused otherwise it is  lost in too much data and so this  attention mechanism will focus both  forward looking and backward looking in  a stream of text it will understand  where to put its attention to in other  words it knows how to train its weights  and predict Things based on its  attention mechanism so it feels  intelligent because of this attention  mechanism and  this really is not turned out it's not  the only thing we needed but it is the  thing that unlocked basically this  entire revolution of AI so to speak this  attention mechanism  now  search engines are actually already very  large attention mechanisms if you think  about it Google's mission is to organize  the world's information and make it  universally accessible and useful  so to not overload us with information  their goal is to sift through the  billions of web pages and find the  handful that are most exquisitely  optimized for what you needed at that  time in that moment attention mechanism  and chat have a lot in common because of  the Transformer architecture  but Google search and attention problems  have a lot in common because the nature  of search is one of attention and so  with that we're making our first  conclusion here Google is very similar  in a sense to an attention problem but  if you look at their mission to organize  the world's information  and to make it universally accessible  and useful not organizing the world's  information how do you do that you do  that in databases you have to store it  somehow now Google cannot on a server  store every character of every page or  every byte you cannot duplicate the web  on its servers that would be too  expensive so it has to compress that  knowledge somehow in an index did you  know that AI models like chat gbt are  also compressing knowledge they are  practicing a form of lossy compression  and this is a way to think about it it  does not store your file or your data  but it stores weights Associated that it  learned from it so you can think of it  as a form of velocity compression very  lossy compression what's an example of  that your MP3 file your JPEG file  because there are certain pixels that I  can't distinguish and there's certain  audio tones the ear can't distinguish  from one another now if you had a  perfect Spectrum hearing an MP3 file  sounds terrible and if you had maybe a  super human eyesight and superhuman  displays you might think a JPEG looks  horrible but it is because the human eye  and the human ear can't perceive certain  stuff that we can afford losing that  information that cannot be perceived  so it is important to draw this parallel  here because a search engine like Google  and chat GPT are both in a way  organizing the world's information  they're just doing it in a very  different way instead of a database a  knowledge graph  GPT is storing it in a weighted model  weighted attention so it is still  compressed knowledge just a form of  compression even more extravagantly  small it is losing a lot more  information and Google does but it is  compressed knowledge nonetheless  otherwise it couldn't pass a medical  exam and give  more accurate answers than the wrong  answers if it did not cram in a sense  while training it couldn't replicate  that information in some sense not as  reliable because there's a lot of lossy  stuff going on here but still it is  compressed knowledge and so we already  have identified a second commonality  between search engines and GPT  moving on we talked about parameters and  we talked about tokens now parameters  are like weights weights is what you  learn more weights means smarter model  tokens are common text sequences and so  you can put things in perspective gpt3  has 175 billion parameters  and almost 500 billion tokens that it  was trained on the tokens is how much  information it sees  when training and parameters is how many  weights it retains how much memory  storage it has so to speak so the more  memory capacity it has the larger the  model  the more weights you have the more  memory and CPU you need the slower the  model takes to run but also the more  expensive it is to run because it  requires more memory and CPU or GPU  Cycles however also how smart are the  output so the the world has been trying  to push these models to have more  weights and more weights and more  weights and as a result you hear a  bigger and bigger models this is not  actually what we want so in a sense yes  but in another sense they keep getting  bigger and more expensive and slower the  bigger these models run  so it is not just the number of  parameters that matters but also how  optimally weighted they are we'll talk  about that in a second but when we  increase the parameters  magic happens and so here is an image  from Google's Palm research paper that  shows what happened when we trained an  AI model that had a lot more parameters  to it and it is like magic because more  centers of cognition are appearing the  larger the model becomes  and it is almost like watching a child  grow up taking his first steps learning  to speak learning to write learning to  do math in school and graduating so it  is very important that these uh models  keep growing in size to unlock new  functionality however it is also  problematic because these models become  too big to run efficiently and cost  efficient  so now we get to our friend Ethan and  Ethan is an edgy personality which means  that he takes somewhat of a bold stance  on this maybe  some might describe it as overconfident  but he is not necessarily wrong in his  overconfidence it is just not proven yet  and he could be right what is Ethan  saying Ethan is saying that scale is all  you need at this point the problem of  intelligence is solved by  Transformers and their attention  mechanism attention is all you need but  to make it more intelligent  Ethan is saying scale is all you need so  he's saying is just blow up the models  make them bigger and to an extent as you  saw in the previous slide that is true  the Palm model  got smarter and smarter as it became  larger and larger but as we discussed  there are problems with that because the  larger these parameters the more weights  you have that means the bigger the model  that requires more CPU and memory to run  and at some point intelligence can  become so smart that it becomes  inefficient to run and so if you look at  the human brain is extremely efficient  using its energy for intelligence and so  until a computer can be as efficient to  run  that intelligence may just be crippling  right we all have seen movies like A  Beautiful Mind where you have really a  hyper intelligent individual  but is seriously impaired in some way in  a social sense  and it is not inconceivable that and a  super intelligence might be achieved  when it is has a very large model at  some point  that just becomes too expensive to run  it it would be much better to just let  humans do the work and when a model like  that is too expensive to run it it beats  the entire purpose because that means  you can't scale it and if you can't  scale it then the idea of cheap compute  on the internet where everybody can have  a website and so and everybody can have  a computer at home and so forth and so  on that idea would be broken scale might  be true the scale might be all you need  to get to Super intelligence it does not  mean that it automatically solves all  the problems we need to solve in order  to have a super intelligent in an  abundant sense  so this brings me to the chinchilla  scaling law as a paper from deepmind a  Google sister company and they  discovered and it's been a little while  ago that data or tokens the parameters  are trained on and not the size of the  model not the parameters it's currently  holding back large language modeling  performance  now what it means is if you have four  boxes where you could store stuff  that was most important to you four  memories  and I only showed you a single room of  the house then you're going to pick the  foremost your attention mechanism will  pick out the four most important items  from that room  if I show you two rooms that's like  showing you more data or tokens you  still only have four parameters but now  you can pick the four most important  things out of two rooms  and the chinchilla scaling law says that  currently we said oh we can make the  model smarter instead of four slots or  boxes let's give you eight let's double  that but I still only show you one room  and so the chinchilla scaling law said  that the ratio of the number of rooms  you saw and the number of boxes you  could store or little items was off it  says that intelligence doesn't just  increase by making the models bigger  but also by showing you more stuff  showing you more rooms to pick important  stuff from  and naturally you can think about this  like Human Experience too adults don't  grow an extra brain it's not like we  have a second and third and fourth and  fifth brain as we grow up let's just say  young person has a fully developed brain  at some point  you don't get a bigger brain just  because you go from 21 to 80 years old  so what does change is your experience  and the more experience you have you can  compare that to the number of tokens or  data you have seen the more you are able  to make decisions wisely and from  experience and the same is true for  these neural networks so there's a lot  of commonality with the human brain the  chinchilla scaling law basically says  that the current generation of AI that  you have interacted with  have not seen enough rooms in the house  and therefore they're not as smart as  they could be  and instead of going from four boxes or  storing four items to eight if you would  keep it at four  but see more rooms you can actually have  a better representation  of what was important in your life  a better attention better intelligence  so the chinchilla scaling law goes back  to that concept of attention it just  says that experience and data matters a  great deal when setting these parameter  weights and making better weights better  decisions better outcomes better  intelligence so what that means is that  the next generation of scale the next  generation of AI may have a smaller  model  but just see so much more data  that it actually outperforms models that  have a lot more boxes but have not seen  as many rooms  now this is very important because  online let's dispel this myth we see so  many articles of people saying oh gpt4  will be 1 trillion parameters let me  tell you I don't necessarily want gpt4  to be one trillion parameters unless the  costs of gpus drops 5X  I don't want our applications to take  five times as long to run I don't want  my cost to increase 5x what we really  want  is maybe a slight increase in parameters  a massive increase in data that it saw  but that would not increase the cost to  run the model  that would just increase the cost to  train  the model  and this is why the expectations that  you see around GPT are not necessarily  good assumptions at this point people  are begging to be disappointed so at the  end of the day nobody cares how many  parameters or tokens are in there nobody  cares whether Transformers were used or  not what you care about is how  impressive the intelligence is  how fast it can answer  and how cost efficient and reliable the  services  that's it you don't care about all the  technical mumbo jumbo but what I'm here  to say is that if we do go to these very  large models they become cost  prohibitive to make available at a  massive scale at this point until  Hardware becomes even more abundant  now there's good news like the cost of  running these models has really been  keeping up with the Innovation so far I  hope that continues and as we are going  to see what scale is all you need the  problem is going to be more efficient  training more efficient running of these  models and more efficient hardware and  breakthroughs in Material Science and  manufacturing that are going to unlock  Super intelligence with the current  hardware and the current technology we  have we cannot achieve the scale that we  need at this time right so really  important for people to understand when  they interact with a chat GPT based on  3.5 there is a limitation in  intelligence that is Hardware bound just  like we have human limitations on how  high we can jump our Hardware or legs  have limitations and so it's important  to note but that's also hopefully this  dispels the myth around gpt4 most likely  it will be a little bit bigger or  available in a little bit bigger  addition  but it will be seeing a lot more data  when training meaning that it could be  vastly smarter than gpt3 at a comparable  price point at a comparable speed of  output which means that the consumer  applications in Search and more are  going to be incredibly accessible and  impressive going forward  but not yet your hyper intelligence so  to speak okay that brings us to a not so  theoretical problem really the alignment  problem let's hypothetically say that we  did achieve the necessary scale to  achieve super intelligence it's just a  matter of time it's not an if but when  and assuming Ethan is right and scale is  all we need what is the next problem how  do you make sure that such an  intelligence does not turn into a  Terminator bent on destroying us  so if you have a very powerful machine  let's just go like a sports car pick  your favorite car and it had no steering  wheel and it was not autonomous a  machine without steering is useless to  humans it would just be  another creature doing its own thing and  so why would we even create something  that doesn't benefit us in some way  doesn't do some work doesn't achieve  something so alignment is what that is  called so making the AI do what you're  asking it to do now open AI has started  that work in alignment from its own  experience so at first it put out GPT 3  in the market and people were asking it  to do stuff and they would say write a  blog or write me an answer to this  question and very frequently gpt3 would  not answer people what they really  wanted openai  retrained their weights  to be more aligned  the output is more aligned with the  input that means that the AI is more  doing what you want it to do and so then  they release the models called instruct  GPT and they have been available to  Developers for a while  and instruct GPT is exactly what it  sounds like when you're asking it to do  something it actually  tries its very best to do that and chat  GPT is just one more step on top of that  and so it uses reinforcement learning  with human feedback loop and so you  could say that the model was pre-trained  but after it is pre-trained it is the  weights are refined based on human  feedback and there's a layer a system  after the pre-training now whenever you  interact with chat GPT it is  additionally learning from Human  feedback and so with Chad GPT what they  did is they took the pre-trained model  they had humans do a manual pass and say  you know this is not safe you should  never say that and these human laborers  have a very high weight to at all and  then every single time you use this chat  gbt there's a massive amount of data  being processed every day of human  feedback and so that is further  reinforcing the weights and telling the  AI despite everything you've read you  shouldn't say that and that was what  missing from from Microsoft's Twitter  bot of your once upon a time they made a  Twitter bot and it turned into this  raving racist that would enable another  Holocaust if it could on Twitter and  Microsoft had to shut it out so in 1010  alignment  are what makes  AI useful  if it's not aligned with your intent  if it doesn't do what you want it to do  then it is not useful it might be  interesting but it's not useful  and that brings me back to Google's  mission statement  Google's mission is to organize the  world's information make it universally  accessible and useful  so my next argument here is that the  reason we are seeing this conversation  is this the end of Google and so forth  and so on is because  the alignment problem and intender line  is the same thing  that Google has been solving for us and  GPT is technology that's just another  way to solve intent alignment  and so this is why you see so many  comparisons between the two technologies  and why people are saying this is the  end of Google or ask the question at  least so we have seen a lot of parallels  so far so let's compare we're going to  call Transformers because GPT and open  AI are not the only game in town right  meta is in the race go here AI 21 Labs  Microsoft had its own models even Google  has its own Transformers so we're just  going to call it Transformers because  there's more than one right there's Al  Optimus Prime there's bumblebee there's  more than one Transformer and let's go  and look at these statements right so  first of all Google is intent on  organizing the world's information  making it universally accessible and  make it useful how does Google do that  it organizes the world's information in  knowledge graphs and databases it makes  it universally accessible via the search  page the home page of Google and the  Google Assistant wildly popular  and making the information useful  and so that they achieved that with  rankbrain which is a form of human  reinforcement learning human feedback  reinforcement learning and every time  you use a chrome or the browser or  Android or a prop visit a property with  Google analytics or double click  tracking on there etc etc Google is  measuring your your user behavior in  reaction to your intent so it is looking  at your search intent and it is  measuring whether it aligns with what  user Behavior once you encounter the  intent it is measuring how much you are  aligned and it uses that to reinforce  its own weights to understand whether a  page is useful for you or not or  contains useful information in other  words Google has this this idea of what  you're looking for Transformers organize  the world's information not in knowledge  graphs and databases but in model  weights you know the parameter count  they stored it as a sort of lossy  compressed information  they make information universally  accessible via chat and prompting  and they make it useful  thanks to alignment research this is  currently the biggest difference  between openai and other Transformer  products is that they have done more  work in alignment research than perhaps  some of their competitors at least the  ones that are publicly available  the current winner for this is really  Google but in the first category of  organizing the world's information it's  a no contest it is an absolute  crushing victory for Google and that is  because Google crawls more information  sees more information indexes more  information and provides more factual  and reliable information than anybody  else and they have  more than a decade of experience doing  that the other two are really close I  would say making it universally  accessible  is a toss-up because we have seen that a  chat interface is extremely useful to  make information accessible right a  hundred plus million people have signed  up for uh chat GPT because it is so easy  to use and accessibility is important  right so this is the first time we have  seen a paradigm shift really in the  interface of how you consume search  since the voice assistant and really  chat is a form of the Google assistant  so it's not that Google does not have  good interfaces it's just that we have  this paradigm shift in how people want  to interact with it also on screens and  then make an information useful I will  say that Google is still the winner  now it is definitely  Up For Debate for some people in some  use cases for example if I am a coder I  might say that the Transformers are  making this data more useful because my  coding problems are not just solved with  linking to a web page where I can find  an answer maybe but it actually also is  giving me code examples and that's  infinitely more useful but on the other  hand it also includes very frequently  wrong data and wrong answers and when it  comes to matters of your life and your  money you cannot rely on that data so in  other words there is a lot left to be  wanting from Transformers in terms of  making data that's useful that is also  reliable and factual and so for that  reason I am still calling it for Google  in this department how are we trying to  let's say we Collective we Humanity AI  AI researchers how are we trying to  overcome these things organizing the  world's information is we're trying to  combine these knowledge graphs  with Transformers  and so you can take you can think about  this as a human being with a general  intelligence giving it a open book exam  and it is the human is then able to make  sure that what it says is in alignment  with the facts in the book but it  applies a general level of reasoning on  that information and this level of  interpretation  and that is something that is happening  right now we'll talk about that a little  bit more making it universally  accessible that means that we are likely  to see chat become part of search but  also search to become part of chat so a  chat that is able to browse the web and  a search page that is able to chat with  you about its web search results  and making it more useful reinforcement  learning with human feedback and a  massive data scale  and solve the pre-trained problem the p  and GPT by adding a form of post  training which is what the reinforcement  learning is trying to add but all it can  do with reinforcement learning with  human feedback is steering the weights  it cannot teach the AI brand new things  at this time but when it can and I  believe it will in the future it will be  an incredible breakthrough for the  industry  so you already see it  um happening Wolfram Alpha suggested to  combine with chat GPT  Microsoft being an open AI are going to  launch an answer engine inside of search  and Google is expected to to make its  Google Assistant available for users in  a similar sense as we said it's like a  human taking an open book exam and it is  a phenomenal way to combine the benefits  of search technology and accessibility  and usefulness and interesting fact is  that on this just a little bit about the  company that I work at ink for all when  Google introduced this reinforcement  learning component with rankbrain around  2016. I was working in machine learning  already at the time and I saw under how  neural networks were changing the game  and I I said that at that time that  neural networks were going to be used to  in influence the quality of content that  you see in Google search results and so  we have at that time we started working  in an agency and created our own agency  and built technology AI technology that  kind of reverse Engineers this process  so at inkforall.com you can this is a  spin-off from that agency that inherited  all the technology so to speak spun it  out we created this ability to look at  the highest ranking results and with  neural networks built a real-time AI  model to understand why is Google  ranking this high so it's not just in  Google that's any search algorithm that  ranks content there is a reason why it  is ranking higher it's because they make  more money if they give you better  search results right that's the  fundamentals or are better product  results if you think about e-commerce  but then when gpt2 and and Transformers  came around it became obvious to me that  Transformer technology would be combined  with knowledge graphs at that point and  so inc4all.com is an AI writing content  creation product that reads your top  performing competitors reverse Engineers  them builds a knowledge graph of facts  and information and then Builds on top  of Transformers to create content that  is more factual more human more  interesting more relevant to your  audience than anything out there and so  we've already got a patent issued fully  issued not not even pending around this  and I'm only mentioning this because you  can believe what I'm talking about when  it comes to this topic because these are  things that I had worked on and  predicted and was so confident that  would happen that we spent money on  attorneys to codify those inventions and  this is the future and the strength of  combining search with Transformers  knowledge graphs with Transformers is  that  you get the best of both worlds so the  search engines  index the web and gather data and  compile information and find out what  what reinforcement learning what people  really care to know  what is authoritative what is  trustworthy and so forth and so on and  then you give that like an open book  to a Transformer  who does not need to know the entirety  of the world and the entirety of the web  at all times when you combine those two  technologies you make magic and so this  is what you can expect to see in search  very soon now somebody asked me are you  a team Microsoft and open AI or team  like Google  and they made a recent acquisition as  well in the in the chat AI space but I  would say that people are creatures of  habits  and Google is still the most impressive  in terms of its Knowledge Graph and in  terms of its reinforcement learning  because of analytics because of Google  because of Android because of chromium  browsers because of a double click  Network you name it YouTube Every  acquisition Google has made has been a  data play it's all been about intent  that's it you can look at all of its  Acquisitions in this space and it's all  been about having more information about  what people really want to put content  that's it and when it didn't turn out to  provide better data then they shut it  down like circles and their social  network Hobbies but I would say Google  is still best positioned to be the  dominant player what is likely to happen  here is that Bing might get a little bit  more market share  but it's unlikely to Dethrone Google  they run the most popular a search  engine the most popular Voice Assistant  the most popular mobile operating system  the most popular browser the the most  popular video platform the most popular  analytics platform the most popular  content marketing and you get my point  Google has more data than anyone else  and I've said this many times before  I'll just repeat it on this video if you  have not heard me say it before Bing and  Microsoft don't have worse Engineers  than Google per se okay it's not like  they don't know how to code or make a  great search engine  the difference between Google and other  search engines is data Monopoly Google  has monopolized the oil so they control  manufacturing and in the search sense  they have controlled the data monopolize  the data  therefore they have monopolized search  you want to break up Google break up  their data Empire  and you will see search becoming more  competitive  and this is what Bing is doing right now  it is a good thing for the search engine  industry it means that Innovation will  happen it means that Google can't get  away with half of their page being a  giant advertisement right Google didn't  used to be that way it just  progressively keeps becoming more and  more monetized competition can be good  for consumers in that sense right so  let's move on one way that people have  done this is by combining search results  with Transformers here you can see two  examples and this is most likely what is  going to happen right you see a search  engine Knowledge Graph combined with  Transformer to summarize the findings  but very important very important here  note how the sources are still there and  that is not going to go away  I will argue that very strongly in this  in this presentation you also have other  examples where chat GPT is a  side-by-side Google search through a  Chrome extension and then people can  upvote and downvote and see and  basically show chat GPT plugin there if  uh chat GPT provided a better search  experience than Google or not  it's a way to train future models  now if you have  paid notice so far my thesis is actually  that  to unlock the power  of a transformer for search you still  very much need to have data  you need the data so the best crawlers  are going to have the most data and  therefore  we should not just look at search  competitive landscape from a perspective  of who has the best Transformer I  believe Transformers are going to be  commoditized and be quite common so the  architecture for a GPT is open source  it's out there you can build your own  GPT if you know how to code the only  constraint is how much time and money  you have to train it and gpts will be  common what is going to separate them is  the data and the world is constantly  producing a stream of new data and the  Crawlers that search engine Bots  commercial Crawlers  and more that are seeing more of the  internet and seeing more data are having  a huge Advantage here  what does that mean it means that you  might see an hrefs search engine right  because they a crawl they crawl a whole  lot that's what this means and yet this  image came from an ahrefs article but  you can see  where Yahoo is Baidu  Microsoft semrush Facebook these are the  companies that are really in the running  to take advantage of this kind of  technology in the context of search  moving on interfaces do matter  so this is a headline from late last  year  where it was reported that Alexa was a  colossal failure on Pace to lose 10  billion dollars in one year as it turns  out  people don't want to buy stuff over  voice  interesting so give them a screen and  they'll buy  right so why have you seen Echoes Amazon  Alexa with screens aha  people wanted to see what they were  buying so interfaces matter  how we consume data matters so Voice or  chat is not a universal interface that  works for every single use case and this  is important to keep in mind humans have  habits and these habits are really hard  to break so phenol may become CD and  then CD goes extinct and we go back to  vinyl radio becomes podcast  doesn't go away it just transforms photo  albums maybe Instagram MTV becomes  YouTube becomes shorts newspapers become  news websites paper letters become email  and search will become  better search so search will not  disappear  it will evolve  the fundamental need to discover  information  remains and the fundamental medium to  discover information search will remain  it will just evolve  it only thing that can replace search  is a better search  remember the attention problem we get  overwhelmed if we have too much  information  with more content being produced than  ever before in human history  is more important than ever before in  human history  that you can filter through that  information with attention  problem solver  and that is the mission of search so  search does not go away  now let's move on to monetization and  the economy of the web and I call this  the content attention economy  so the deal is that we allow search  engines and AI to index our data  and make it accessible for profit make  no mistake Google is making billions of  dollars on your information collectively  in exchange  they send traffic to our sites  what happens  if they stop doing that you stop sharing  your information with them  that means they can't operate a search  engine  chat GPT  is no different  if they are not able to access your  information they can't be smart  and if they can't be smart they have no  economic use case  therefore  economy dictates that the backlink  the traffic Source from search  cannot and will not go away  that means that content marketing will  not go away GPT does not provide sources  for knowledge right now  and  it cannot replace search engines for  that reason and it is not likely to be  in the interest of open aai  to become a search engine I think openai  very much prefers to remain a research  company  that creates models and this AI research  but other people can use open AI  technology and again other Solutions  exist Transformer technology is more  accurate  others can use it to integrate it into a  search type of problem solution that  does have attribution right so GPT on  its own will not replace or be the next  generation of search it may be combined  with other Solutions in an interface  that is an evolution of search but it  will not replace search and not on its  own and when it is combined with a  search engine type of solution  it must  contain backlinks otherwise it is not  economically viable it will die out  people will start putting their sights  behind paywalls  and it will be the end of the billion  dollar money making machine that is  searched  then there's the ownership problem we  have seen this before in news  even a backlink is sometimes not enough  of a payment  the European Union has granted  Publishers additional rights over their  content with consent based criteria for  inclusion and use in products like  Google News what does that mean cons  what does that consent look like  pay up  that's what it looks like  so news outlets Why Pay news outlets  versus your website your website may not  contribute brand new information  or your website may not sell products  if your main product is new information  like news outlets  you don't get any benefit from Google  taking your information reading your  article and then reorgetating it online  without sending you a subscriber or  money and so forth the ownership problem  is real and we're going to see a lot of  litigation worldwide related to the  ownership problem and chat GPT  because chat gbt when and if allowed to  read your news article  it is not providing attribution an  attribution as you see is not always  sufficient payment  and under the EU regulatory framework  it needs to be consent based criteria  that govern this kind of operation so do  not expect open AI to all of a sudden  have access to the latest news in its  latest model unless they start paying up  just like Google had to do  then there's the memorization problem  which is just here to illustrate how the  ownership problem can be an issue for  Transformers and and you can see  stability AI has a memorization rate  it's a very small memorization rate but  it does have a memorization rate and  this is just to once again underscore  that that these Transformer models these  weights are really just compressed  knowledge all right if it was not  compressed knowledge the image on the  right would be impossible to achieve so  it's very lossy but in some cases it can  be more lossy or less lossy than other  cases interestingly too uh stack  Overflow band chat GPT generated answers  why  they said it's too often inaccurate but  actually chat gbt Among Us training  sources is reported to have been trained  on stack Overflow the interesting thing  about this is that stack overflows data  included both correct and incorrect  answers huh interesting so if its  attention was focused on correct answers  only perhaps chat GPT would have been  better but I have personally seen that  chat GPT generated answers that are very  close to an answer I saw on stack  overflow on the same problem  fascinating  who would have thought that it would  follow the patterns of the data that it  was trained on so combined memorization  as a problem the lossy compression  component with the problem of ownership  and problem of attribution of the in the  attention economy and you get this idea  this really good idea that  you can't really divorce attribution and  the economy of ownership from this new  Transformer technology it is something  that has to be figured out  we are talking about data monopolies and  so how do you differentiate in a world  where everybody has this kind of GPT  Transformer style intelligence that the  quality is going to be determined by the  quality of the data and so this here  I've already made the case that search  is not going to go away  the search will just be replaced by a  better search I've already made the case  that the contenders for the next search  interface are the ones who have access  to the best crawling data and so what  I'm underscoring with this slide here is  that  the only competitive Advantage you can  have  in such a technological future here is  data  and so expect companies to look for data  monopolies people who have access to  exclusive data or exclusive insights are  going to have the ability to create a  smarter AI  and so one such data Monopoly that  you're seeing is for example the real  world  the real world is a Uncharted Frontier  for robotic AI Teslas Robotics and an  autonomous driving projects are really  good examples of data monopolies  surprising perhaps but mapping more  streets than everybody else or in the  case of a humanoid robot mapping  interaction with the real world and  experiencing physics is data as well and  so the company that gets a head start on  such data has a competitive advantage on  the web  we are going to see a Resurgence of  paywall communities companies like  substack where you have trusted authors  that do not share their data with the AI  Bots and the only way you can get that  new information  is through such  membership to such sources also  manufacturers who make products they are  the first to have the information about  their new car so they are going to  perhaps control how that information  goes out to the public and create a data  Monopoly or use their data in a clever  way to exchange it for attention in the  attention economy so if you are a  company now and you have access to data  that only your company has that is a  monetization play for the near future  but actually there is more to this  because AI will get to a point where it  can also invent net new information and  we already see that in in biological  research remember Transformers excel at  translating sequences to other sequences  and your DNA sequence or your protein  sequences are biological sequences and  Transformers are also really good at  that so they can sequence and simulate  medications in your body and accelerate  medical developments with ai ai is able  to solve math problems and much more at  some point AI will just be inventing  or B take a role in inventing almost  everything going forward  so we know at a certain time human  knowledge becomes public domain  and what if after some future time  AI contributes to almost all future  human knowledge then that will become  public domain because it looks like at  least right now ai systems cannot patent  inventions  that was recently confirmed by the U.S  federal circuit court  all right so is the future of knowledge  open source is the future of knowledge  public domain trade secrets are going to  become important but trade secrets are  only going to be possible if you have  access to Unique data and bringing this  back to search who has the best data on  the web on information who has the best  data  Google does Google has a data Monopoly  therefore Google will not be dethroned  anytime soon and so that is my thesis  around this and why I believe these  things SEO is not going to go anywhere  because as long as people search there's  a need for optimization we have a new  knowledge economy and so knowledge that  is human and novel will retain value for  a while  knowledge that is common knowledge or AI  invented will either be kept trade  secret or become public domain  and at some point  some point in the future  when all information becomes public  domain search engines won't even need to  ask your permission they will not need  legal attribution  for sending you traffic for what is  considered public domain however  they will still do it I believe they  will still do it because removing  attribution  will also kill the content attention  economy meaning that search engines  would no longer get access to your juicy  novel new human information and do not  forget there's the consumption side of  content as well as long as humans  control the dollars and humans control  the attention what they spend time on  right the data that humans produce is  not only what they write  but also what they choose to consume and  engage with and so search engines need  those humans for a good while longer  so this is not going to go away even if  it from a legal perspective wouldn't  have to be necessary it will be  necessary from a data perspective  and so I want to point out that these  Transformers and these Technologies are  not just restricted to text there are a  lot of companies I could shout out here  11 Labs has made phenomenal text to  voice and voice cloning technology that  is in many cases indistinguishable from  Human so expect unbelievable things  about content  unbelievable things in terms of quality  of content Flawless AI does a similar  thing with lip syncing it is is  translating text changing text and  making the actor say different things  without reshooting it there's virtual  influencers infinite future is a World  of Warcraft with whatever evolving  storylines that are personalized to you  endless Seinfield episodes they are  quite bad right now but they are going  to get ridiculously good and you can  imagine pick your favorite Universe  Marvel Comics Star Wars what not and  being in there fully immersed and you  can imagine that immersion to increase  with with virtual reality  like Sword Art online and eventually  creating infinite fantasy worlds in  whatever universe or metaverse you want  to lose yourself in so these are  things that are really happening so the  world of content is going to change  quite a bit in the future and what you  may not be able to believe it right not  everything you'll be able to believe it  because how convincing and compelling it  will become  so let's talk about what AI cannot do  well AI can't bleed it lacks The Human  Experience  the rush of adrenaline falling in love  becoming a father  the first hand Human Experience is  something an AI cannot provide  and that means that marketing needs to  become a lot more human  and empathetic  so what do brands do  in light of all these changes  well  search will evolve  it will not go away  it will continue to be essential and it  will continue to be a major traffic  source for your business as long as  they're a search there's need to  optimize content traffic will continue  to go to the best content the most  trusted Brands who provide a superior  experience and seos will continue to  optimize for such outcomes the job of  SEO is to optimize client outcomes and  navigate them through the changes how  Brands build Authority optimize their  content so that their users experience  quality well that's it for this  presentation but there's a lot more to  say about the topic and I hope to see  you again in the next video where we're  going to break down some of the things  as they are developing I hope it helps  you and I'll see you in the next one bye