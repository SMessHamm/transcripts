 welcome to the future of Life Institute  podcast my name is Gus docker on this  episode I talk with Anders Sandberg from  the future of humanity Institute at  Oxford University  we start out by talking about chat GPT  which is the newest language model from  openai but we also talk about in general  what we can learn from how AI has  developed about the future risks and  benefits of AI I really enjoyed this  conversation and I think you will too  here's Ander Sandberg  Andrews welcome to the podcast thank you  for being here well thank you so much  for having me  all right I assume you've been playing  around with the chat GPT  um the new language model from openai we  tried it of course I have it it's a very  fun very shiny toy right now  what what have surprised you about its  capabilities  I was probably most blown away when I  read that you could actually use it as a  fake Linux terminal to make it pretend  that it's actually a computer and you  can move around in different directories  install software surf the web except of  course it's not the real web it's a kind  of a dream web but it's imagining and I  saw people tweeting about it so I tried  it myself and I got curious what other  files I'm finding on this computer so I  found this uh folder containing uh  letters so I found the CV of the owner  of account so I read the John dose and  the the CV where he's talking about his  skills as a programmer at XYZ  Corporation  it was a weird feeling because in some  such GPT is making up a very plausible  file to find on a generic computer from  indeed a very generic person  at the same time I felt like maybe I  shouldn't go and look at the private  files because we're probably private  where is this information coming from  maybe we could explain how GPT line  models work  so generally the large language models  are glorified versions of the text  completion we have in our smartphones  you basically try to predict what is the  next word or even the next letter coming  from a previous sequence of characters  and it's apparently obvious that you can  do this in a simple statistical method  but you just take a lot of text  calculate the probability of a next word  based on the previous ones except that  that doesn't do much for grammar and  indeed this kind of random Markov chain  models have been fun sources of nonsense  poetry and since the 1970s in computer  circles what the language models do is  they add a more complex artificial  neural network that actually is aware in  some sense of previous words that have  been in a sentence if I start talking  about animals in a sentence then various  words that might be ambiguous might be  more or less likely given that the  context is animals if I switch to the  context start talking programming then  suddenly other words become more likely  this is all there is to it it's  basically predicting a sequence of  tokens  based on a large Corpus of text that you  train it on and now it just predicts the  next token and then you can of course  add more to that and it generates a  fairly likely text  this is in itself really amazing because  some of these texts actually do seem to  contain real meaning and then when you  do it interactively like in chat GPT  you're writing something and you're  getting a plausible response and it's  very hard to shake that feeling but  there is intelligence here I give an  instruction and the text responds with  following that instruction even though  what's actually going on is not so much  but there is a being that understands  that others want me to do these things  but rather it's a text that is being  updated by two actors and the game is  very much like if there is an  instruction earlier in this sequence  then that should be applied later on so  there is no real understanding in one  sense in another sense it's very clear  that I can ask these systems which  colors are more similar to each other  and I get reasonable responses  this is weird because of course these  systems have never seen an effect they  have only ever expensed a lot of text  but that text comes from humans but  generate of course a lot of color and  appreciation a lot of color names a lot  of color comparisons and given that it's  more sensible to say that green is  closer to the maybe a turquoise than  read this  then the text system will do this too  the problem is this kind of reflected  intelligence it confuses us very very  easily  yeah I think it's important to remind  ourselves how these models actually work  sometimes it's  it's  confusing to say I I had a chat with a  GPT uh the in the weekend and I had a  discussion on moral philosophy with this  model and it felt real to me it felt as  if the Marvel understood what I was  saying and then I I I asked it to  correct the point that it made because  it was wrong and it acknowledged that it  had made a wrong point but then it  repeated the point over and over again  which breaks the illusion so it it's  also amazing to think that how broad the  knowledge in these models is so I know  something about moral philosophy but you  could also ask it about asking about  engineering Bridges or art or poetry  whatever it is so in one sense as you  say  these this line of models show  understanding but in another sense it's  it's brittle understanding  it's also not sensitive to  inconsistencies which I think is very  interesting I I saw somebody that had  had a wonderful argument with chat GPT  about whether it could speak Danish and  the model was responding in Danish  saying no I don't understand Danish I'm  trained trained on an English Corpus it  was obviously wrong obviously  inconsistent and it didn't care because  that was the most likely way that where  the conversation could continue  it's interesting to think about these  abilities that to us seem uh like a  qualitative leap but they arise out of  basically the same type of model that  just has more data or has more  parameters or has longer training times  more expensive training runs but to us  it seems as if the models are learning  new things yeah for example that that's  we can now hold longer conversations  yeah what do you think this trend will  continue and we will continually be  surprised  I think it's a very good bet that we  will be surprised because our ability to  predict the capabilities of AI systems  has been demonstrated again and again to  be really bad it goes both ways people  have both been too bullish about how  easy it would be to actually program  many tasks and discovered the hard way  that computer vision is a really deep  problem but solving some problems that  are easy for young kids is actually very  hard for robots to do well it's not  terribly hard to write software but does  symbolic math that is very much more  powerful than any human could ever be so  that's more of EX Paradox that the  five-year-old is kind of outperforming  the best robot uh while the rather  simpler programs are outperforming the  professor but at the same time we're  also being wrong about things not being  doable  so in the current year of large language  model there is a lot of people who know  a lot about the language and AI that  have been making very confident  predictions about their failures and  shortcomings and that this cannot be  dealt with and usually the story is a  few months later a new version shows up  and it totally blows those problems out  of water and then typically the human  expert gives another interview where he  proclaims some other problems being  really a good test to show that this is  not true intelligence and that it cannot  possibly be solved and my bet would be  that six months later a new version  arrives and thus even that even though  the underlying basic point but yes the  language models don't really have  understanding in our sense is valid it's  worth recognizing that yes we're not  intelligent in the sense we use about  having a goal having  plans to implement physical the language  modes are just predicting strings of  tokens but we're doing it so well but  even people in robotics have started to  use them for internal communication and  planning because you can of course use  these strings of tokens and to also make  up good plan how do I make a cup of tea  have the language model describe the  steps send that to another language mode  that controls the robotic arm  so then you combine the instructions  from the instructions from a language  model with some computer vision and then  you have the beginnings of a robot that  can actually navigate in the world  yeah  so if we look back if we think say 10 or  20 years ago we perhaps would have  thought that we would have a pretty  pretty high functioning self-driving  cars by now  but that didn't turn out to be the case  at least not yet but we didn't predict  or at least I hadn't seen predictions of  language models with this kind of  capability uh as we're seeing now  can we use this information to predict  how we will be uh surprised in the  future or is that too much to expect  I I think we can use it as some kind of  analogy so generally history doesn't  repeat itself but it Rhymes so the  autonomous cores were a beautiful  example where he got the kind of  transition back in 2006 when DARPA did  its first Grand Challenge  the chorus we're not doing very well I  think none of the cars actually managed  to get the goal post even though they're  driving in an open desert  and most people were laughing and saying  yeah that's uh how bad it is  people who really paid attention said oh  this is actually getting somewhere next  year they actually all went uh and they  reached the goal post and progress  seemed to be very fast and indeed  shortly they were driving in urban  traffic  so that led us to believe that oh we're  going to get autonomous cars very soon  now  then it turned out that there was  another problem yes of course you can  make a car that drives relatively well  in a city or at least on a highway  dealing with unexpected circumstances  and making it safe enough that we can  entrust it to drive our children  that is a very different thing so right  now people are a bit more pessimistic  about the autonomous cars because  they're good enough to demonstrate that  they can be unsafe and actually getting  that extra level of safety so we would  say this is perfect to use is very very  hard  meanwhile the language models were kind  of flying under radar because most of us  didn't even think it was very important  I said they were glorified the cell  phone tax predictors  and in one sense that's exactly what  they uh or except that were so good at  predicting these sequences but you  actually can use them to solve problems  um gpt2 could with some effort to play  chess since there is enough chess games  in chess notation you still need to do a  bit of extra work to just get legal  moves I bet you can do much better with  chat GPT this is not because it's doing  a proper chess evaluation yes but the  database contains so much and generally  the big surprise to somebody like me who  learned about neural networks in the  1990s before they really worked well  before they were cool before you got a  six-figure salary for knowing this is  but the internal structure is not  fundamentally dissimilar from what we  were tooling around with it's just that  you have much bigger data sets and much  bigger compute to actually do the  training so many of the things I  solemnly told my students about black  human night is like oh yes you need to  avoid having two many degrees of freedom  in the network because you will get over  training and that's going to be very bad  turns out to be not really true these  days instead you want to have a lot of  parameters you want to have billions of  parameters because you're both training  on a much bigger data set but where even  this surprising properties where over  parameterized neural networks seem to be  broken that is actually the term that  people are using in the field now that  we find the underlying to the pattern of  a domain and then can be a very good  accurate predictions so grok is from  high lines science fiction Noble the  Moon is a harsh mystery so and a boy  reared by martians has this weird  Martian word for truly understand  something to actually be that thing and  it seems like in some situations the  underlying complexities of reality deep  down have certain patterns that you can  learn once you get enough data this was  not something we could predict but  neural networks would be good at and  indeed in the 90s my advisor told me  about the yeah deep neural networks  they're kind of a dead them the way  didn't get much more results than  original tests in the 80s and that was  totally true with the 1990s and they  stopped being it through in 2010s  because we had enough data enough  compute to run big Networks now this  story about a mixed bag of predictions  seems to suggest but over the next  decade we shouldn't be too surprised if  it turns out that there's some new  applications or architectures that  become very powerful and they might come  from almost any practical application  also that all the methods can be used  for other things so for example the  convolutional Deep neural networks that  were used in computer vision  you can do one-dimensional versions of  them to make sound syntheses and  language synthesis so we can totally  expect that as a spin-off of a working  computer vision we're going to get much  better voices and probably artificial  music that's a likely prediction  but which jobs are going to get  disrupted  Flipper die flip a coin you don't really  know uh when the Michael uh the Osborne  called Frey did their famous study on  the vote the metability of jobs that was  published in 2016 and they claimed that  47 of all jobs could be automated in the  foreseeable future  they gave probability for different jobs  and it turns out that artists and  illustrators verse only four percent  chance that it could be any automation  but of course the 2022 saw this rise of  AI art where Dali 2 and a mid-journey  and stable diffusion really scared of  our community because it's pretty clear  that at least some of Illustrator jobs  are totally getting automated meanwhile  the Insurance Underwriters that we gave  a 99 probability of they could be  automated or doing fine why well  actually their job is much more about  business relationships than actually  taking numbers filling them into  spreadsheets and calculating a premium  so it turns out that our ability to  reliably predict the consequence is raw  below and that leads to a strategy it's  kind of recognized that yeah we are  going to get surprised here and that in  itself might mean that we need to hedge  our bets quite a lot  do you think the failure rate of  language models will hold them back from  it from from being a from being used in  the world to solve actual problems like  we mentioned with the self-driving car  there the failure rate is too high for  them to actually drive our kids around  you could imagine a language model  lawyer but if that lawyer as a language  model  um every 100 Clause it produces this  complete nonsense then we can't really  trust it in a way that we perhaps can  trust humans so do you could you could  there be similar problems where language  models will confabulate or hallucinate  uh on say on true things and therefore  they they can't be used as productively  as we wish they could  yeah I think it's very likely in many  domains so the automated lawyer it's not  a problem if every 100 paragraph is  gibberish because real lawyers can also  occasionally say gibberish and it  doesn't matter  if it's a if a lawyer says something  about this aren't true on the upper hand  uh oh now they are in real trouble  and the problem is of course language  models are true bullshitters in  frankfurt's uh the the philosophical  sense they don't care about being true  or false they just generate an output  so this makes them very unsuited as they  currently are for that kind of job I I  tried out to chat GPT on chemistry  experience I asked it what happens if I  mix the following chemicals and besides  a short lecture that I shouldn't be  doing that because it's unsafe which was  totally correct it gave a somewhat  correct answer for two chemicals and  then I changed it to another chemical  and again I got a short lecture but  that's an even more unsafe experiment  which was correct and then they gave a  totally erroneous answer and it was  really convinced about this erroneous  chemistry that would not be very useful  if I actually wanted to do proper  chemistry you need to tie it the output  the actual facts in the world much more  strongly but on the other hand I tried  it to come up with scenarios for  role-playing games I pointed out that  well classic Dungeons and Dragons is in  a fantasy setting but maybe we could do  other historical settings so that first  I had it come up with good names for  spells in a classical Greek setting and  then I asked it could you come up with  some other interesting historical  settings and it suggests that the 1815  France the post-napolionic era and we're  discussing the possibilities of having  under the emperor loyalist soldiers this  was creative work it was not super  creative it was not coming up with  something that was totally unthinkably  weird but at the same time it was useful  I could tell it let's add a complication  let's add the Marshall of France  currently the king of Sweden to the  storyline and that worked out quite well  this is creative work there is no right  and wrong it's only entertaining and  less entertaining and this is where I  think it can be very powerful  if I were to use one of these models as  my personal computer as personal  assistant I really need to know that it  does what I tell it to do it needs to  actually demonstrate that it sent off  that emails and doesn't just hallucinate  but of course a set of the email sir you  actually want to be able to check that  is reliable enough but different domains  have different kinds of reliability I  can totally see people using this to  generate the large amount of text where  we don't care too much about reliability  a lot of the copy a lot of report  bureaucratic reports which are probably  going to be read by another language  model distilling it down to a shorter  forms of the rest of bureaucrat will be  certain that I have done the right thing  for the grant I got and it might be that  now we're just using these models to  send the enormous amount of verbiage but  no human ever reads  but maybe if that saves our human nerves  a nice uh that might be a good idea  it is weird that these language models  will output text that is completely  false with the same confidence or as as  they will output through the true  information so as a user if you're if we  imagine a user using this as a kind of  search engine to try to find true  information well then you have to be a  good judge of whether the AI is  bullshitting or whether it is  regurgitating true facts that it has  learned online and that I can see that  holding the models back because we in a  sense the models can be trusted to tell  the truth  well the reason is of course what are  they trained on they're trained on Reams  and reams of human text and discussion  and conversation and humans are not  super reliable even the scientific  literature is full of falsifications  errors and misunderstandings so the  reason unexperienced scientist is  somewhat reliable at their job is that  they have built an internal model that  is fairly consistent and is quite often  tested against reality  the problem with the text models is of  course they don't really care and they  don't currently have a method to test  against this  so we need to add more things to make  them really useful so I would expect the  current crop of text mods are going to  be just generating text where truth is  not the most important part the Next  Generation that might be powerful is of  course the ones that link to extra  semantic information about the world  where they actually check the  mathematics if we make equations and  make sure that it's a correct  calculation  now automated fact checking would be  really powerful imagine if you had an  assistant but constantly pointed out  when you read something whether it  actually is at variance with known facts  or not that could be very powerful  although a lot of people would  absolutely not want to hear that because  a lot of known facts they believe they  know are not true indeed it turns out  that quite a lot of facts that even  experts believe are not true there is  actually a disturbing uncertainty about  many of the things we as a civilization  believe so you it's by no means given  that even these fact Checkers are going  to bind the actual truth sometimes we  might even act against it because well  the official view on what the state of  the world is might actually be incorrect  but only in the right way I think this  can be very very powerful  the real problem is of course A lot of  people are just going to use it naively  or recognize that it's unreliable but  it's uh the the five to five o'clock and  they want to get home and they need to  write 10 pages of text let's have a  language more generated let's skim over  it and yeah it looks good enough and  let's send it off  so we might very well end up in a world  with a lot of unreliable verbiage which  we already live in of course but now we  get much more of it the funniest part is  of course but you also get a lot of  weird lies from these models because  they are trained on what humans output  and if you ask a human are you a robot  most people will argue no I'm not a  robot and will give various arguments  that is of course part of a training  data there is a lot of examples online  of that so of course the language models  tend to respond unless you specifically  train them not to respond like that by  pretending to be human this can  sometimes be very amusing and sometimes  downright creepy we have a lot of his  accidental lies because we have a system  that's not a human trained on data  generated by humans and before long  we're going to have very different kinds  of systems and entities that might  actually be needed to train or to  recognize what kind of entity they are  what claims we can and cannot make this  is complicated and it's an interesting  issue because it's not just programming  it's prompt engineering and to some  extent even the sociology of  understanding what is this system for  imagine if these language models begin  generating a lot of text because it's  simply it's so easy for them to generate  a lot of text at the request of of a lot  of humans and then further imagine that  this text is uploaded online and  included in the training set of the next  language model then you could you could  see how the ball could get rolling on a  very weird  um hallucinatory model where the the AI  is learning by data that previous AI  models have created and so maybe maybe a  a new field is hunting for authentic  human text  oh yeah uh I think this might actually  be quite important there is already some  work by the people developing image  generation models that they should have  watermarks so they can recognize that  they should not be part of a training  data there are some natural governments  that have started to say that yeah you  need to start adding watermarks uh to  what is being generated and part of that  is of course not polluting the well  because one scenario I envisioned is a  spamocalypse for example today if you  want to get a recipe online it's very  likely that the top recipe you get will  have a preamble about five screen pages  about a lovely holiday in Sicily and  those lovely lemons before you get to  the lemon meringue recipe at the bottom  why well of course it's being monetized  there is uh those impressions as you  scroll past the ads at the side that  actually make them a site pay for itself  now writing that the verbiage about the  holiday that nobody should really read  that much that could be done by a  language model  but also recipes can can't be  copyrighted they're not creative texts  they're a list of ingredients so  actually you can totally steal that from  another site but you will probably want  to rewrite it a little bit so we don't  harass you too much and I can totally  foresee that of course an unscrew the  scrupulous web designer would say yeah  I'm going to use language mod to make  recipes and preambles and then I get to  add clicks and then I'm making money and  then of course the next day and they are  going to be doing this for something  else than recipes so the internet might  get filled with this auto-generated  context  pretending to be human and also of  course becoming part of training data  which might mean that now the language  modes are getting very confused about  what ingredients actually are in apple  pie  because if you get random noise in there  at some point maybe carrots should be  part of apple pie the model doesn't know  it just includes it  so you could get an erosion of our  shared epistemic system here if this is  not handled well so obviously you want  something that checks the fact against  reality and you might also want a flag  that this was probably auto-generated  but we might also want to think about  the whole systems we have about recipes  or scientific papers these epistemic  systems are kind of fragile and the  incentives for maintaining them well are  not always well aligned  I don't know if you grade undergraduate  essays at Oxford but uh does this  support you that that you will you will  have essays handed in that have been  drafted by a language model or perhaps  you know handed in directly from a  language model do you think this is an  actual threat or are they not Advanced  enough to to fake and Oxford  undergraduate yet  I think they're not good enough to fake  an Oxford undergraduate he said with  some pride but uh that is right now and  I fully expect that I'm going to see  some of these essays before in fact it  would be a good thing if the philosophy  faculty sprinkled in a few  auto-generated essays just to check that  the faculty is actually alert enough  if the problem I think with  auto-generated philosophy is is that  we're based on what is in the big Corpus  what everybody knows or at least what  somebody has argued so it's not going to  break new ground normally you don't  expect undergraded essays to do that and  that is the big problem so once you get  up on the Master's level and PhD level  you might actually want to demand that  this should be unique stuff but nobody  has actually started before and that  might be possible to even check for but  if you just have somebody explain what  how Epicurean ideas affected the  Machiavelli  yeah I think the language mods are going  to do a decent job on this so at that  point the obvious solution we need to do  a proper philosophy tutorial where I sit  across from a student Oscar so can you  explain what the epicurus and  Machiavelli ever had to do with each  other  that might be the only way and it might  very well be that the student says yeah  but shouldn't I be allowed to use a  language model to partner partly make my  arguments after all over in engineering  we're allowed to use pocket calculators  and I think there is some Merit too it  might be that the right kind of models  might actually improve things I would  love to have an argument validity  detector for my own text but reads the  text and checks does this make poor  model sense and actually warned me when  I'm kind of deducing too much hand  waving and that might be useful that  would be a tool just like a pocket  calculator that we should want to have  but the earliest application or of  course going to be cheating I have no  doubt whatsoever that a lot of very bad  scientific papers are being generated  right at this moment are being sent to a  lot of journals for review by people who  really want to pad their CV  and we need to deal with that we need to  find better ways of actually checking  the papers are useful but they are true  but we have validity that experiments  were actually done  um and that's going to be tough going  this reminds me of this mirrors a debate  about you know you have a say a third  grade student learning how to add or  subtract how to do basic math and and  then this the discussion is around well  I will always have a calculator and  they're right they will always have a  calculator in their pocket uh we will  always I'm guessing have these language  models available and they will only get  better so why should we artificially  handicap our own Thinking by not using  them and perhaps the answer there is  that well we are training human ability  so we are we're training critical  thinking or ability to to do math  mentally and so on and and if if we oh  if we Outsource our learning to these  tools well then we will never gain the  human abilities  that could be one answer of course the  skeptic might say yeah but why do I want  to have that ability and my answer to  the math student would be it's actually  very useful to check in to check the  sanity of your answer if you can do a  quick math in your head quite often that  is an extra Safeguard that's very useful  to the philosophy student you might say  yeah but don't you actually love wisdom  don't you actually want to figure out  for yourself what is true  after all you're not going to become  rich by your spouting off about  Machiavelli and Benton you actually need  to have some proper understanding  because the language models are dealing  with the surface level of a message very  a very good model of what uh other a  text would be like but that text is  supposedly reflecting some underlying  understandings and Concepts now it seems  like the language modes are good enough  that they almost imitate this deeper  level well enough and indeed quite a lot  of our languages we use every day is  probably not smarter at all than the  language model we're just generating  words but fit with this situation and  where I bump into a chair and I  apologize to it but not because I'm  thinking that the chair is an individual  but I need to apologize to it's just a  reflex because well I bump into somebody  and something and then they see  responsive to apologize  but getting that deeper level that  semantic information those meanings that  is tricky and we might actually want to  have systems that tie more closely into  that especially when you want to have  True full language models gpt3 you could  actually improve it through fullness by  asking it to give a truthful answer  because you need to point out that we're  interested in the part of language that  deals with true statements rather than  the big part of language that's  arbitrary ones but you might also want  to hardwiring things but it actually  checks the numbers it's giving against  known data in the world that would be  working on a deeper semantic level of  course we might also train on a lot of  things and hope that maybe these models  will grock the deeper semantics of the  world in which case we're actually  approaching some better understanding of  the real thing but in many cases there  were many deeper levels we might be  having a conversation about one topic  but actually the real reason we're doing  a conversation might be a political game  in our organization or something very  different actually understanding what's  really going on can have quite a lot of  levels some of which are unsaid some of  which might even be unknown to the  participants they don't really know why  we're engaging in some kind of social  game but there are reasons for that we  really want to have a model that might  be able to act on those levels and the  deeper they get of course the closer we  get to real intelligence  but I think the language models are  going to be a bit like the pocket  calculator very good for making texts  look good I make a sketch of what my  argument is and then they fill out the  details and make a nice rhetorical  flourish I'm proud to admit that I  actually did this a few months ago with  the paper I couldn't come up with a good  in the end point of a paper uh it was a  simple one about plastic recycling  nothing fancy but I couldn't come up  with a good rhetorical flourish so I had  GPT free give me a few Alternatives I  choose one and added that for the first  iteration of a paper  it didn't contain enough science it was  no research it was just a nice send-off  of this is why we think this is a useful  method and it's going to help solve the  waste problem but expressed in a nice  way but you might want to have a deeper  levels too you want that fact check here  you want the math to check here you want  to have maybe a system pointing out to  you by the way this argument you're  giving is very related to that book you  read last week maybe a very solid  correlation here  do you think that there's anything deep  to this notion of understanding or will  it kind of dissipate when we find out  that  the future language models can can in  fact simulate what we would deem to be a  deep understanding of a topic do you  think do you think that do you think  when you say uh when you talk about  whether a language model understands  something do you think we mean the same  thing by understanding  yeah I don't think normal human  understanding and language model I  understand they are the same thing they  work in slightly different ways so when  I say that I understand let's say some  equations in astrophysics that typically  means that I can use them I can use them  to calculate things I can explain what  happens if you change the equations a  bit I might know something about the the  range of parameters where they're valid  and so on I might be able to tell you I  found this story about how they were  discovered  now this is not just making a simple  prediction about the output language  models are very good at making these  predictions of output but they generally  don't have that causal connectedness  generally what the current machine  learning is weak at is understanding the  causal effect I do something something  happens they're thinking all in terms of  correlation and this quite often works  well as they say a correlation does not  imply a causation but as some weight  added but it certainly winks  suggestively the if things are  correlated with each other they usually  have something to do with each other  quite a lot of stupid errors happen when  you assume one causes valver quite often  there is something hidden beneath or  behind that of course both to happen  together if you can discern that then  you have understood the situation and  can do much more interesting things like  changing that underlying factor in the  useful ways this is something I think  language models right now are unlikely  to be good at but again their General  sequence prediction tools it might be a  very risky to say they can never ever do  this it might very well turn out that  with the right kind of training or the  right kind of applications they can  actually start finding correlation  structures  I think Judea Pearl and many others  would say no way we're not buying that  then give very very clever Arguments for  why this cannot happen and it wouldn't  surprise me but it might generally not  but I but there might be domains where  it actually works well enough but you  can get in a cheap causation  understanding and that's good enough for  those domains but it might also be that  we have a very important domains where  this absolutely doesn't work and you  think a totally misled we need to learn  those the difference the different  domains and the boundaries between them  do you think that language models  abilities to generate lists of of  actions so for example give me a list of  the things I need to do the actions I  need to take to get a cup of coffee well  okay get the cup boil the water put the  coffee in is that a very rudimentary uh  understanding of causality I don't think  it's a understandable causality it's an  understanding that humans explaining how  to make coffee and tea make it in this  order humans generally I assume have an  understanding of causality when we're in  the kitchen and then they recount on how  they did it and then they don't explain  how they concluded but you need to pour  in the water after bringing the cup it's  just always there so then you get a  correlation structure that the system  learns and now it looks like it has a  causative understanding and it's even  General enough that if you say I don't  have a teacup in my kitchen can I use a  glass it will probably say yeah you can  use a glass bring the glass pour in the  water  you do get something that is very  similar but it's a bit deceptive because  it's actually not based on understanding  the causative structure now a lot of our  language is full of assumptions about  causality so many that I think it looks  down what the language model can say in  such a way that it actually borrows our  sense of in the causation and I think  this is generally what happens with the  language models there is so much human  output but you can actually borrow all  of this and use that reflected  Brilliance to look like you actually  understand the things you can steal  human common sense and give it back to  us so that it looks like you have common  sense in air exactly yeah yeah yeah of  course how many of us also look very  intelligent actually we just read a lot  of stuff and listen to podcasts but  actually uh in a conversation we bring  up some nice Bon Martin uh so cool fact  and it sounds like we are very very wise  but actually yeah it's the rest of  mankind channeled through us  I think that's a very common experience  we should talk so you've hinted at this  or talked about it but we should we  should think about how these models will  evolve how they'll get more grounded how  they'll gain semantic understanding  perhaps how they will become multimodal  so will they incorporate  images or  um video and so on it could what what's  future directions could these models go  in  so the real issue is not so much what  you want to add but just how much can  you add and do you have enough compute  for it so making a multimodal language  model seems totally obvious just add a  camera and a microphone and now you get  a data stream which in many ways is  similar to just getting the text Data  stream  there are some issues about noise about  training but basically we're talking  about patterns of data and finding an  architecture that's good at handling it  probably you might say yeah I'm going to  put in some model like open a as whisper  uh to actually parse the language  because we already worked rather hard on  that so instead of just having  unfiltered sound my multimodal image is  going to look for the camera and hear  what people are saying although having  the raw Sound Stream might also be  useful because the chirping of birds and  the the Bowing of dogs actually is also  part of that world and then presumably  it would do a good sequence prediction  and be able to predict in a scene what's  supposed to happen next  but the problem here is more like okay  can you get enough training data so  right now we have been using that idea  that we can get enormous amount of  training data get it suitably labeled  and then produce a nice output the  problem is of course getting cheap label  training data is tricky I think we run  out of that thing people used to say  that data is the new oil and right now I  think we're approaching a kind of weird  oil crisis having a really good curated  repository of good training data is not  trivial to get your hands on would this  data have to be human labeled or could  we have ai labeled training data for AI  with some quality control mixed in could  we could be set up this training set so  that it wouldn't pollute the model and  degrade performance but would instead  perhaps expand performance improve  performance I would bet that you could  do interesting things here because you  could start by having a model trying to  label the data a human responding and  the expensive and rare human  interventions come and train an internal  critic that helps retrain the actual  labeling model  um this is something that has been  discussed a fair bit in the AI safety  and the AI Alignment World because there  ideally you would want the robots and  the AI systems to always do what humans  want them to do or make sense but humans  are so slow and it's so expensive to ask  humans for advice so instead what you do  is you try to train on a human giving a  bit of scarce advice  so you get an internal model pretending  to be that human giving advice of the  same style and then try to see can this  trade well and it seems to work  surprisingly well I don't know whether  this could work in these applications  but it's certainly worth trying and I  would be very surprised to hear that  nobody is doing it right now  how have your opinions about AI safety  changed because of how AI AIS have  actually turned out so perhaps in the  90s we didn't have that many examples of  actual AIS and perhaps now we do I don't  know if that's an actual uh of if that's  an accurate picture but that's that's  kind of my summary of the history here  now we have some examples of simple AIS  uh have you understand how have your  opinions changed about how we can make  these models safe  so back in the 90s very few if any  people were really thinking about AI  safety it's well worth noticing but  people in the 1950s were claiming that  oh within a generation we're going to  get human level Ai and we're not  concerned about safety at all there's  this very famous research proposal where  they this is in I think 1956 and they  proposed that with a give a small group  of researchers at summer and they will  make significant progress in artificial  intelligence research that's that's the  start of the field where with a very  overconfident prediction but go ahead oh  yeah the dwarf move conference that this  was about was also accurate they did  actually do a surprising amount of  useful stuff over summer because there  was nothing before when there is nothing  any Improvement is infinite Improvement  but they were very optimistic about  getting powerful Ai and even some people  like Alan Turing and IJ good were aware  that well once again at self-improving  AI that's going to be very dramatic but  none of them really seriously talked  about the safety issues and part of it  might be that they still had that  distrust because when you see how badly  your actual system work well you have a  hard time believing that it could be  powerful enough to matter and very it's  a lot of cheap talk  in the 90s I was part of a transhumanist  movement we were all very bullish about  the imminent technological singularity  and this is going to be great and we had  members of the mailing lists argue that  we need to speed up the singularity so  we should really be working hard to make  an artificial intellencies that could  act as a seed AGI to improve itself and  it's actually from those conversations  we got a lot of early safety thinking  because some of the people involved sort  of realizing wait a minute if we're  taking this absolutely seriously I  believe that we could get this  exponential growth of something becoming  smarter and smarter that means that we  are essentially creating a good and its  values are going to be set by whatever  we happen to program and at this point  any programmer realize oh I mess up all  the time and oh if I mess up programming  something good like that's scary  that led to a conversation about how do  you actually make something that is safe  and from some of these discussions you  actually end up in the earliest part of  the AI safety debate but it's worth  noting this was not part of the  mainstream this was among the weirdo  futurists meanwhile the AI resources in  the 90s they were mostly interested in  making sure that the industrial robots  don't squish people that had been a  concern since the 1970s and it's a valid  relevant concern but AI being used for  any super important jobs in the world  they knew the field well enough to think  that this is not very likely to be a  problem right now we can wait  and I was probably very much in that  world even though I was one of those  optimistic futurists AI seemed fairly  far away I was much more optimistic  about many other Technologies  and again it turns out that futurists  are very bad at predicting which  technology arrive early so while life  extension is going on nicely but a bit  too slowly for my taste uh nano  technology got misdirected by the  sociology of research funding while  biotechnology is charging on but so  unpredictable we can't tell anything new  space is way ahead of what we would have  expected while AI for a long time didn't  do anything and then suddenly just  jumped and became much more of an urgent  issue and that made me concerned about  AI safety I've always been much more  optimistic than many of my friends in  this field uh perhaps just because of  Personality rather than facts and good  reasoning but it seems to me that yeah  we're getting systems that can amplify  you my performance tremendously that's  already enough to be a course of serious  concern it also looks like we are bad at  predicting when the performance can  increase and even what they're doing  once they have a higher performance  these are very good reasons to work very  hard on designing them better finding  better ways of validating and testing  them and understanding the consequence  of a society the problem right now seems  to be that the field is a bit divided  between AI Ephesus that are looking at  the standard ethical issues that you get  from these systems and they're rightly  concerned about the bias in algorithms  and how power accumulates but they tend  to totally dismiss concerns about the  power of AI systems themselves and the  transformative power while the people  working on the transformative AI side uh  they or to some extent dismissing the  everyday ethical issues because they  seem to be so small compared to the  really explosive power of a powerful  system so unfortunately we have ended up  with two communities that don't talk to  each other enough and they don't get  along enough to actually be helpful  because it's pretty obvious that if you  can make an AI system but can understand  human social mores well enough not to be  racist  then that is also a helpful thing for  figuring out a system that can  understand human social mores enough to  be somewhat value aligned with humans  and that might not be an issue of just  trying to change the training date that  might require some Robert deep  architectural discussions it might also  be that we need to think a lot about  governance and long-term strategy  especially if AI is going to become very  transformative I think many of the  current discussion about the aifx are  going to be blown out of the water  simply because just too much happens  as we saw with AI art in 2022 that shock  really shocked an entire community and  let a big debate and we're going to see  this probably every year for the next 10  20 years where different groups that  never expected in their whole life but  AI would matter to them suddenly  interrupts there and it becomes  Paramount  there is this core technical issue in AI  safety of aligning an AI model to a set  of values any set of values where the  goal is just  for any set of values can we make it so  that the AI pursues these values for  example we could call it the values of  humanity as opposed to some random  values but there's also the the  philosophical side of deciding which  values we should have the the AI pursue  and you quickly get into difficult  discussions if we're thinking about now  say we have an a language model that we  have trained and  improved over time so that it does not  output racist content perhaps the same  the same techniques could be used to  have a language model that that does not  uh criticize various governments for  example and then you you start you start  to see how they could be worries about  authoritarian governments using this to  read the internet and pick out uh  what I'm saying is that we have  conflicting  um we have conflicting goals here and  some of them will will collide with each  other and we and there's they are super  difficult questions about which values  to to give to these AI  yeah we don't know what the True Values  we ought to have are if and it's an  ongoing debate I think we made some  progress over the last 2500 years but  yeah it's a complicated issue uh  similarly what kind of rules do you want  built into the AIS that is also a real  important thing it's a matter both of  power and utility uh for example many of  the image and rating systems don't make  naughty pictures and you can kind of  understand that open a I don't want to  generate that because we're a company  they don't want to mess around with the  issues of political propaganda and the  pornography so they carefully trained  daily to not to do that which had other  interesting side effects because if you  remove not the pictures from the  training data that means that you  actually remove a large number of  predominantly female people  which means that you get a gender bias  which they corrected for so in this case  we have a bit of an understanding of a  particular bias but it probably also had  other effects that we don't even notice  because we don't even understand what's  going on and you can imagine of course  making AI systems that don't uh help you  do dangerous things but what is  dangerous  and this goes back to the more deep  philosophical debate about for example  freedom of expression many outrageous  ideas have turned out to be true so it's  actually a good thing to be able to  express outrageous ideas and it might  very well be a good thing to have ai org  systems that can make in both the sexual  and political pictures indeed if you go  to a fancy art gallery or Art Museum  typically the artworks that you know the  curators and the reviewers talk the most  about are the ones that probably the AI  program would not allow to be made  so it might very well be that we want  diversity here and that actually goes  back to the interesting value alignment  problem because it seems like one very  valuable approach to this is to  recognize that we are uncertain about  this and the AI ought to be uncertain  about the two we might say we want you  to make people happy and then hopefully  the AI system recognizes that you must  don't quite on me know what they mean by  happy and uh we and make us also is a  bit uncertain and actually they might  not like my in a proposal because their  literature is full of dystopias that  pretend to be Utopias and stories about  AI progress doing bad things  um when they're giving good commands I  should be carefully so you actually want  to learn many of these things about an  axiological uncertainty we're uncertain  about the value is normative uncertainty  we're even uncertain about what morality  to do uncertainty about the world our  place in it Etc  what does this uncertainty do well it  softens thing if you're absolutely  certain as an AI program what you should  do you will just do it and that seems to  be very dangerous AI programs that are  good at handling uncertainty are  probably not going to go ahead wire  quite as strongly  now this still leaves us with another  big AI save the problem that's AI  programs doing what you must tell them  to do but you must happen to be  malicious or misguided or stupid and  there's a lot of those people out there  and if their power get Amplified we're  going to see a lot of trouble but at  least we have some ideas about the  valley alignment problem and uncertainty  but they go well together and one of the  most interesting things about language  models is that the text we see if that  is just the thing that is being  generated it's the most likely words  following each other a bit randomly but  there are many alternative ways the text  could have gone we might want to have  tools to actually see the alternative  versions of a text to be able to  backtrack and say actually let's follow  this other branch of thinking or this  other branch of style  I think we actually can develop better  tools for interacting with these systems  so it's not just a question about  training data or having bigger computers  but also getting better training data  and better tools for interactively  working with them some of the best  images I've seen generated by Machine  learning AI art  or done by having them make a few  pictures Advantage human paints over a  section says over here I want the house  or this part of a uniform should be a  bit more science fictional and that  phase make it Sterner and then you  iterate you have an interactive work now  this is where I think chat GPT is such  an interesting tool because it's not  just generating a text one time you're  actually having a conversation you're  nudge it in different directions it's  still by no means perfect it's very easy  to get it into totally inconsistent  state but that interaction is also a  good way of conveying now this direction  is the wrong path we might want to go  down another path  I agree that having AIS be uncertain  about values is an is an interesting  research Direction but I worry that it's  simply easier to to make an AI that has  a more hard-coded or precisely specified  utility function it might be easy but  it's also likely to fail in Annoying  Ways so imagine that you're running your  authoritarian government and your  program your AI to follow your official  ideology do you now get on AI but you  are aligned with probably not because  MILF authoritarian rulers actually don't  behave according to their own ideology  and this comes back again and again to  buy this indeed NSA probably felt that  Snowden was a very trustworthy  individual because it was going on about  the Constitution all the time and we're  so aligned with the Constitution and its  values which turned out that Snowden  disagreed with so similarly that the AI  with black and white utility function  might very well say my authoritarian  government is not obeying its uh  Sinister in ideology well enough I need  to make it do that which might be a bad  news for the leadership of that  government so a well-defined utility  function it quite often backfires in  interesting ways I I imagine that we're  surviving authoritarian governments  would also say yeah we want a bit more  uncertainty it should recognize that  with some nuances and subtleities in our  Sinister agenda so it shouldn't just be  treating it black and white  that's one direction to go true true  Anders uh thank you for talking with us  thank you so much  that's it for this episode on the next  episode I talk with Anders about his  upcoming book Grand Futures which is  about what Humanity could do at the very  limits of physics