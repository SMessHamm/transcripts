 hello Community let's talk about the  future of two trending topics chat GPT  and stable diffusion  so let's have a look at this  chat GPT Renault you have a prompt and  the system provides a text for you  it could be a lyric it could be an essay  it could be an instruction to do  something you give it a prompt and you  have here text to story  currently runs on Tubidy 3.5  the other one is stable diffusion  version 2.1 we are here now in mid  December 2022 and you have a technical  prompt and you get out a synthetic image  and if I say synthetic image I really  mean it because look this is the prompt  that I need  according to stability I to get exactly  this image so here we have two systems  where you have a text prompt providing a  text story and a technical text prompt  providing an image now you might ask hey  why you show us those two things well  have a look at this  now a lot of people are trying to  optimize their technical prompt for  their synthetic pictures they want to  have beautiful pictures pictures they  imagine so the technical prompt can be  quite complex  therefore you have hundreds of YouTubers  trying to find out the best technical  prompt  or to be more professional you hire  people we have a micro gig economy with  platforms  where you have people you give them it a  prompt you have now stable diffusion  executed gives you a picture a data  labeler qualifies the picture it goes  back to this label data trains now in  your network or GPT this system now  produces another improved prompt and you  have here a learning system beautiful  so let's look at the company structure  of those applications  chat GPD we have openai a for-profit  corporation  founded late 2015 by Sam Altman Alan  musk beta Thiel Amazon somehow they  collected 1 billion US dollars in 2015  and in 2019 they received 1 billion US  dollar from Microsoft airport is  currently located to my knowledge in San  Francisco  stable diffusion and stability AI the  company and their headquarters are in  Notting Hill London United Kingdom  and if you are interested here in this  micro gig economy and the data platforms  that are specialized to provide data  labeling and data  annotation  given micro tasks when they just cost  about five cents three cents there are  different companies those human gig  workers or human labelers are primarily  sourced by scale Ai upwork and a lot of  other companies scale an eye and upwork  are mentioned by jet GPD in their  documentation that their human labelers  or sourced from scale Ai and upwork  let's have a look at the current  possibilities what are people doing  right now  they tried to use chat GPD to have a  prompt to get out a technical prompt now  the problem is that the system was not  really trained to do this so the  performance is really mediocre a better  way as I showed you is you go to this  micro gig platform and you pay I don't  know 8 cents per task and you let it run  for 1000 prompts and then you take the  best prompt that you get back so  but what we can do is you have a prompt  for example tell me a story about a  beautiful girl who I don't know find a  prince they found a beautiful house and  then they married  and so the chat GPT now generates the  text the story for you and I've seen  that sentences are taken out now and  with this sentence is now the technical  prompt for a stable diffusion to  generate to each sentence or each second  sentence a picture  so we get kind of a storyboard and you  know if we have a storyboard we already  have the technical capabilities to have  here a picture and then we can make here  for example this person smile with  slightly moved ahead so we got little  film strips  from the house of also Sunset and  there's a beautiful marriage so yes we  have here a little film strips and you  know where it is going  you can produce little clips  beautifully absolutely synthetic  so current possibilities easy you riding  on a Harley Davidson on Mars and this is  a clip of 10 seconds no problem this is  Within Reach what we can achieve you  have stable diffusion in painting you  have to morphing mechanisms so state of  the art you can generate those synthetic  movies  little clips  what is the near future if you have a  separated technological evolutions of  those two companies and of those two  technologies  now it is easy chat GPD will become jet  gpt2 it was super or whatever and I'm  quite sure because if you look at the  the first chat the day will have  professional editions for science GPT or  Finance GPT or medical GPT and the  backbone of course will be the new gpd4  so there I think it's just a refinement  for the filters further development of  the algorithms and some very dedicated  system because you want to grab Market  size in each of the main markets great  stable diffusion  well we will have stable diffusion 3  with a specific landscape Edition so you  will have a portrait Edition with  Optimum synthetic  portrays from human or from your pet or  from your loved ones or whatever  and landscape and cityscape and whatever  so I think this is a clear normal  Evolution if you let these companies run  without any further disruptive input  now in order to understand what is going  on let's have a look at the theory and  the code behind this now in stable  diffusion I have four videos I did  and here the first one here stable  diffusion the blue one  I show you the theory behind stable  diffusion variational autoencoder the  Latin space we use a unit we use a clip  text encoder and here's the complete  Theory  here I'll show you the code in pi torch  where I have a free collab jupyter  notebook and I code with you here where  I Channel Auto encode a unit clip and  regenerate a picture really step by step  we build it up from scratch  now before we had the diffusion process  we had the vector quantized variational  autoencoders  and some of the theory of variational  autoencoders I need here when we move  from Vector quantized variational  autoencoders now to the diffusion  process  and here in this diffusion process  there's one  YouTube video of mine  where I show you the images in between I  had a problem how those images in  between are generated but more about  this in a second  so these are the four videos for stable  diffusion if you want to understand the  theory and the code that you are able to  code with me  chat GPD I have two videos  here I have jet GPD and flan T5 flan T5  is currently one of my preferred llms  from Google  and if you want to see here the  performance enhancement we can do with  code this is the video for you and if  you want to focus exclusively on jet GPD  this here is my new video where I go in  detail into the step the theory how does  jet GPT work what are the algorithms  what are the methods to use  how they implemented what is the process  from the very basic up to the synthetic  text or the synthetic instruction or the  synthetic whatever  and you see that here we are talking  here about synthetic images and here we  are talking synthetic text and you can  guess where I will go with the further  of this video so what I wanted to focus  here the images in between this was  something that really struck me you know  because I wanted to have here a look  how is it possible that those videos are  generated and let me show you this is  nice in canva I can go into my YouTube  videos in canva in the presentation mode  and here I am this is what uh no thank  you not this  how can it create new picture objects  how is this possible and the answer is  easy give them sufficiently expressive  neural networks the variational auto  encoder Latin space can fix complex data  distribution very neatly  so you can expect to see and this is the  most important part here smooth  transitions between the different types  of data point in the Latin space  and I think this is for me this was the  point when I understood how those  synthetic pictures could be generated  if you want to have a look at the video  I highly recommend it I think it is  instructive if you understand the theory  because you know you have the astronaut  riding a horse here in the middle so you  have a picture of an astronaut you have  a picture of a horse but never happened  in history that there was a picture  taken with an astronaut on a horse on  the moon so how is the system able to  syntheticize those data  so in the same schema you will encounter  now the other side with jet GPT  do you remember that GPT that showed us  they are training their model in three  steps the first step was here we have a  prompt a prompt from a database and a  human labeler  now writes here the answer to this  prompt let's say the prompt is write a  poem like Shakespeare here William  Shakespeare a beautiful man and our  result is here  a beautiful poem like Shakespeare maybe  she looked it up in in some old English  book or whatever so here we have now our  labeled training data set the response  to this prompt  so label data set by human we are  talking about supervised learning and  with this new data set we can now fine  tune an already pre-trained GPT  Transformer  that has red I don't know let's say all  of the internet beautiful  now please don't think of this as a  classical template because I've seen  comments by viewers say oh this is a  template no it is not there is no  template it's an advanced form thinking  of it as a path you want to follow and  it will make sense if you apply Game  Theory to this so this is Step number  one okay beautiful step number two  remember  we have now our GPT 4 Transformer and  the same prompt now goes as an input to  the gpd4 Transformer and we get four  outputs they are all different we have a  bigger smaller smaller and a huge output  response  and now we have again a human labeler  and this human labeler ranks now the  result from our gpd4 Transformer ranks  it  you notice now another time we have a  human bias coming in because how she her  or whatever ranks it depends of course  on her knowledge on her preference on  her views on her beliefs how where she  is living how she sees the world and  whatever applies for all other person on  this club beautiful  and then when she ranks it the system we  have another data set we have another  training data set now and this is now  the new Trader set training data set for  our reward model for our r model  so this is Step number two  beautiful we have now two neural  networks trained on this and you might  say okay but how is this analog here to  this unseen and untaken picture of an  astronaut riding a horse now easy  imagine this was just Shakespeare  now step number three step number three  is we have a prompt never seen before by  this system and now we let the system  learn and train beautiful so let's do  this so the new prompt is now write a  poem like and not Shakespeare but  Shelley  1792 the United Kingdom the the Great  Britain you know what I mean  and the system has never seen this and  has not been trained fine-tuned on  Trellis but what we have we have our  supervised and fine-tune gpd4  Transformer from Step One  that has in the pre-trained step already  read all of the internet so our gpd4  Transformer  has read all the works of Shakespeare  and Shelley  and then it was only fine-tuned on the  poem structure of Shakespeare beautiful  so here we have an immense knowledge  base  but we are lucky because we have another  model neural network a Transformer based  neural network I I don't know if it's  exactly a gpd4 I have found no  literature about this but just imagine a  neural network and we have just  fine-tuned our neural network here in  Step number two on the best sample of  Shakespeare remember we had four samples  generated by our gbd4 Transformer and we  say hey this is the one I like the most  and this then this and a number four is  really not that prominent  so  and this has been the best examples of  Shakespeare and now we have the beauty  that we can apply it also to Charlie so  let's have a look at this remember we  were talking about here the policy  gradient and openai did some fantastic  work they developed their own PPO their  own policy gradient the optimization and  they use this here that this two system  now learn from each other  we have we are now in the topic of  reinforcement learning from Human  feedback  and this is exactly what happens this is  how you can generate now an answer to a  prompt write a poem like Chalet where  there has been no fine tuning on  Shelley's work it was only stored in in  the uh it's not a data Lake in in the in  the cache of our gbd4 so you see  this is also like here  we have here the work Von Shakespeare  and we have fine tune you know  Shakespeare here we have Shelley but a  combined picture is now like in the  picture case in the image case where you  have the diffusion process now more or  less we have the same process going on  with here a policy gradient approach  which is of course based on Markov  chains but more about this later so you  see this is the way to go and we have  here  now the way explained how these untaken  pictures are generated by the system and  we have here I have this unfin tuned  answers on a prompt stories that are  produced by jet GPD and maybe jet gpt2  or generated with reinforcement learning  from a human feedback great  beautiful to show you the common  technology that both companies use that  both application use  let me step a little bit further out of  the system this 10 000 mile View  with stable diffusion we use here a  Markov chain to gradually convert one  distribution into another  an idea you that's originated of course  in non-equilibrium statistical physics  and what we do more or less we have a  generative Markov chain which converts a  simple known distribution here the  simplest is of course a gaussian  distribution into the data distribution  using a diffusion process  and if you just have those two sentences  those are the most most important  sentences we are operating with  distribution and diffusion processes  from non-equilibrium statistical physics  and then we Define a probabilistic model  at the endpoint of the Markov chain and  since each step of the diffusion chain  has an analytically evaluable  probability the full chain can be and  this is the most part for if you are  mathematician analytically evaluated  and as I showed you the variational  autoencoder Latin space can fit complex  data distribution of our data very  neatly and this is how it is done  you know the Delta of the probability  distributions but more about this in a  second  if you want to know the pure  mathematical structure of variational  autoencoders you have a lot in space you  have a prior probability distribution  you have a family of encoder  distribution a family of decoded  distribution you have an encoder neural  network and a decoder neural network and  you have a loss function and this loss  function has of course the  Reconstruction error the classical error  and then you have the pullback library  Divergence  keep this in mind because we're gonna  use this later on  the other part as I showed you in my  video is here the unit unite now very  simple terms is for the visual part and  the diffusion process and clip of course  is our classical typical text encoder if  you know expert you have an idea what  clip does  beautiful  the coding of all of this kind of go  yeah I can go in is here this is a  Jupiter notebook a column notebook it's  free  I explained what is stable diffusion  and I run with you through all the  different elements  I hope to show you here  diffuser inference pipelines and we  really code here a complete process of  stable diffusion so if you're interested  in a coding here with different  scheduler this is the video for you  beautiful  now coming now to the other side to Jet  GPT  I have here this video where I showed  you  how Jet GPT Works can I go in this video  too yeah beautiful  and here yeah let's stop here  now  they have given us from instruct GPT  there's three-step process and I go in  details through the process and try to  explain what's going on but the most  interesting part is they use the  reinforcement learning approach from  Human feedback and you know the  algorithm that is really famous in this  also a control logarithm or an  automatization algorithm is policy  gradient algorithms  now you know that open AI have a very  specific policy optimization the  proximal policy optimization of a PPO  and from those they have two subsets but  let's forget about this just want to  show you that they also use  the pullback Lively Divergence here in  their objective function and it is  interesting because you start to see  that we use here more or less the same  mathematical instruments and if I tell  you there's a beautiful paper about the  optimality and approximation with policy  gradient methods and a Markov decision  process which is here significant part  of the policy gradient algorithm  you understand that the mark of decision  process here from the left side is  simply an extension if you want in a  mathematical way of the Markov chain of  the generative Markov chain here of the  right side  so you get an idea that the future  development could be on the  technological level  either those companies go here strictly  separate paths and they specialize here  in text to image and hear text to story  or somebody wherever  I mean these are multi-billion companies  so they have to there's a lot of  investment at those companies so they  have to provide profit to the  shareholder and stakeholder but  let me show you an idea if you take here  if I've shown you that jet GPT and the  diffusion the Latin diffusion or the  stable diffusion mechanism they have  some common theories some common  denominator some common mathematical  structure  if you think about this and you remember  that I had a short while ago a video on  simplicial complexes on graph neon  redworks what I showed you in this video  If we go beyond the classical message  passing in graph neural networks and we  focus on simplicial complexes on higher  topological structures let's say in a  topological space  or if you go in an abstract space you  have cell complexes  I show you here why we should go beyond  grapheneal Networks  what are their problems and what are the  possible solution to this  and also another input I would like to  show you here I have a video on the  neural semantic information retrieval  system and here I code review a complete  neural information retrieval system  based on sentence Transformers  now you get the idea I suppose where I  want to go with all of this  because  if you combine this information with  here the graph approach yeah now it is  to I have to you do another video on  graph on the computation on on Pi torch  geometric and djl on deep graph Library  one of my next video will be on  the link prediction but not in a  classical low dimensional euclidean  space but we will go in a higher  dimensional more complex topological  space but I have to show you how to code  this  maybe I have a video I will make a video  about link prediction in pi torch  geometric and I show you the code on the  other side of the paper and in deep  graph Library maybe in tensorflow  that you can see that both Frameworks  are focused on the same idea and that we  need this idea if we want to have a  technological  discontinuity here in the evolution of  chat GPT and stable diffusion so there  will be a new video on Python's  geometric about a higher dimensional  topological version of this and yes I  already showed you a document where it  is not just about variational  autoencoder but about a higher  topological diffusion variational  autoencoder but this is all the hints I  want to give you and there will be a new  video I hope you enjoyed this one and I  hope to see you in my next video